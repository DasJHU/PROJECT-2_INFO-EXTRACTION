# -*- coding: utf-8 -*-
"""PRIMARYSYSTEM_PROJ2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kI2F-F7iu8HcGHLlTMEMJjuZGoGEKS7U
"""

import numpy as np
from collections import defaultdict
from scipy.special import logsumexp
import matplotlib.pyplot as plt

def load_dataset():
    #Load training and test data with label mappings
    with open('clsp.lblnames', 'r') as f:
        lblnames = [line.strip() for line in f.readlines()[1:]]
    label_map = {label: idx for idx, label in enumerate(lblnames)}

    with open('clsp.trnscr', 'r') as f:
        train_words = [line.strip() for line in f.readlines()[1:]]

    with open('clsp.trnlbls', 'r') as f:
        train_labels = []
        for line in f.readlines()[1:]:
            train_labels.append([label_map[label] for label in line.strip().split()])

    with open('clsp.endpts', 'r') as f:
        endpoints = [tuple(map(int, line.strip().split())) for line in f.readlines()[1:]]

    with open('clsp.devlbls', 'r') as f:
        test_labels = []
        for line in f.readlines()[1:]:
            test_labels.append([label_map[label] for label in line.strip().split()])

    return lblnames, train_words, train_labels, endpoints, test_labels, label_map

class HMM:
    def __init__(self, num_states, is_silence=False):
        self.num_states = num_states
        self.is_silence = is_silence
        self.transitions = np.zeros((num_states, num_states))
        self.emissions = None
        self.null_transitions = {}  # For exit probabilities
        self.exit_prob = 0.25 if is_silence else 0.2

    #Initialize HMM parameters
    def initialize(self, unigram_counts, smoothing=1):
        # Initialize transition matrix
        if self.is_silence:
            # SIL model topology
            self.transitions = np.array([
                [0.25, 0.25, 0.25, 0.25, 0.00],
                [0.00, 0.25, 0.25, 0.25, 0.25],
                [0.00, 0.25, 0.25, 0.25, 0.25],
                [0.00, 0.25, 0.25, 0.25, 0.25],
                [0.00, 0.00, 0.00, 0.00, 0.75]
            ])
            # Adding null exit from last state with 0.25 probability
            self.null_transitions[(self.num_states-1, 'exit')] = 0.25
        else:
            # Letter model left-to-right topology
            self.transitions = np.array([
                [0.8, 0.2, 0.0],
                [0.0, 0.8, 0.2],
                [0.0, 0.0, 0.8]
            ])
            # Adding null exit from last state with 0.2 probability
            self.null_transitions[(self.num_states-1, 'exit')] = 0.2

        # Initializing emission probabilities with smoothing
        total = sum(unigram_counts.values()) + 256 * smoothing
        self.emissions = np.ones((self.num_states, 256)) * smoothing / total
        for y in unigram_counts:
            self.emissions[:,y] = (unigram_counts[y] + smoothing) / total

        # Normalizing to ensure proper probability distributions to sum to 1
        self.emissions /= self.emissions.sum(axis=1, keepdims=True)

class WordModel:
    def __init__(self, word, letter_models, silence_model):
        self.word = word
        self.letter_models = letter_models
        self.silence_model = silence_model
        self.states = []
        self.transitions = None
        self.emissions = None
        self.null_transitions = {}

    # HMM construction for composite HMM for word model
    def build(self):
        self.states = []
        state_offsets = []

        # Adding initial SIL
        state_offsets.append(len(self.states))
        self.states.extend([('SIL', i) for i in range(self.silence_model.num_states)])

        # Adding letters
        for letter in self.word:
            state_offsets.append(len(self.states))
            self.states.extend([(letter, i) for i in range(self.letter_models[letter].num_states)])

        # Adding final SIL
        state_offsets.append(len(self.states))
        self.states.extend([('SIL', i) for i in range(self.silence_model.num_states)])

        # Initializing transition matrix
        num_states = len(self.states)
        self.transitions = np.zeros((num_states, num_states))

        # Connecting components with null transitions
        prev_offset = 0
        for i, offset in enumerate(state_offsets[1:]):
            if i == 0:
                # Connect initial SIL to first letter
                for s in range(self.silence_model.num_states):
                    self.null_transitions[(prev_offset+s, offset)] = self.silence_model.exit_prob
            elif i == len(state_offsets)-1:
                # Connect last letter to final SIL
                last_char = self.word[-1]
                for s in range(self.letter_models[last_char].num_states):
                    self.null_transitions[(prev_offset+s, offset)] = self.letter_models[last_char].exit_prob
            else:
                # Connect letters
                char = self.word[i-1]
                for s in range(self.letter_models[char].num_states):
                    self.null_transitions[(prev_offset+s, offset)] = self.letter_models[char].exit_prob
            prev_offset = offset

        # Adding internal transitions
        for i, (model_type, state) in enumerate(self.states):
            model = self.silence_model if model_type == 'SIL' else self.letter_models[model_type]
            for j in range(model.num_states):
                if model.transitions[state,j] > 0:
                    self.transitions[i, i+j-state] = model.transitions[state,j]

        # Initializing emissions
        self.emissions = np.zeros((num_states, 256))
        for i, (model_type, state) in enumerate(self.states):
            model = self.silence_model if model_type == 'SIL' else self.letter_models[model_type]
            self.emissions[i] = model.emissions[state]

#Compute forward probabilities
def forward(obs, model):
    T = len(obs)
    N = model.transitions.shape[0]
    log_alpha = np.zeros((T, N))
    scale = np.zeros(T)

    # Initialization of forward prob
    log_alpha[0] = np.log(model.emissions[:, obs[0]] + 1e-300)
    scale[0] = logsumexp(log_alpha[0])
    log_alpha[0] -= scale[0]

    # Forward computation in log space to avoid underflow
    for t in range(1, T):
        for j in range(N):
            log_trans = np.log(model.transitions[:, j] + 1e-300)
            log_alpha[t,j] = logsumexp(log_alpha[t-1] + log_trans)
            log_alpha[t,j] += np.log(model.emissions[j, obs[t]] + 1e-300)

        # Handling null transitions
        changed = True
        while changed:
            changed = False
            for (i,j), prob in model.null_transitions.items():
                new_prob = log_alpha[t,i] + np.log(prob + 1e-300)
                if new_prob > log_alpha[t,j]:
                    log_alpha[t,j] = new_prob
                    changed = True

        scale[t] = logsumexp(log_alpha[t])
        log_alpha[t] -= scale[t]

    return log_alpha, scale

# Compute backward probabilties
def backward(obs, model, scale):
    T = len(obs)
    N = model.transitions.shape[0]
    log_beta = np.zeros((T, N))

    # Initializing beta
    log_beta[-1] = 0.0

    # backward computation in log space to avoid underflow
    for t in range(T-2, -1, -1):
        for i in range(N):
            log_trans = np.log(model.transitions[i, :] + 1e-300)
            log_emit = np.log(model.emissions[:, obs[t+1]] + 1e-300)
            log_beta[t,i] = logsumexp(log_beta[t+1] + log_trans + log_emit)

        # Handling null transitions
        changed = True
        while changed:
            changed = False
            for (i,j), prob in model.null_transitions.items():
                new_prob = log_beta[t,j] + np.log(prob + 1e-300)
                if new_prob > log_beta[t,i]:
                    log_beta[t,i] = new_prob
                    changed = True

        log_beta[t] -= scale[t+1]

    return log_beta

# Training with baum welch
def train(train_words, train_labels, letter_models, silence_model, iterations=25):
    # Group training data by word
    word_data = defaultdict(list)
    for word, lbls in zip(train_words, train_labels):
        word_data[word].append(lbls)

    # Initialize accumulators
    accum = {}
    for letter in letter_models:
        accum[letter] = {
            'trans': np.zeros_like(letter_models[letter].transitions),
            'emiss': np.zeros_like(letter_models[letter].emissions),
            'null': defaultdict(float)
        }
    accum['SIL'] = {
        'trans': np.zeros_like(silence_model.transitions),
        'emiss': np.zeros_like(silence_model.emissions),
        'null': defaultdict(float)
    }

    log_likelihoods = []

    for iteration in range(iterations):
        # Update accumulators
        for key in accum:
            accum[key]['trans'].fill(0)
            accum[key]['emiss'].fill(0)
            accum[key]['null'].clear()

        total_log_prob = 0
        total_frames = 0

        for word, lbls_list in word_data.items():
            model = WordModel(word, letter_models, silence_model)
            model.build()

            for lbls in lbls_list:
                total_frames += len(lbls)
                la, scale = forward(lbls, model)
                lb = backward(lbls, model, scale)
                total_log_prob += np.sum(scale)

                # Posterior calculation
                lg = la + lb
                lg -= logsumexp(lg, axis=1, keepdims=True)
                gamma = np.exp(lg)

                # Accumulate counts
                for t in range(len(lbls)):
                    for s, (typ, st) in enumerate(model.states):
                        accum[typ]['emiss'][st, lbls[t]] += gamma[t,s]

                for t in range(len(lbls)-1):
                    for s1, (typ1, st1) in enumerate(model.states):
                        for s2, (typ2, st2) in enumerate(model.states):
                            if model.transitions[s1,s2] > 0 and typ1 == typ2:
                                prob = np.exp(
                                    la[t,s1] +
                                    np.log(model.transitions[s1,s2] + 1e-300) +
                                    np.log(model.emissions[s2, lbls[t+1]] + 1e-300) +
                                    lb[t+1,s2] -
                                    scale[t+1]
                                )
                                accum[typ1]['trans'][st1,st2] += prob

                # Accumulate null transition counts
                for (s1,s2), prob in model.null_transitions.items():
                    typ1, st1 = model.states[s1]
                    typ2, st2 = model.states[s2]
                    if typ1 == typ2:
                        null_prob = np.exp(
                            la[-1,s1] +
                            np.log(prob + 1e-300) +
                            lb[-1,s2] -
                            scale[-1]
                        )
                        accum[typ1]['null'][(st1,st2)] += null_prob

        # Update models
        for letter in letter_models:
            # Update transitions
            trans = accum[letter]['trans']
            rowsum = trans.sum(axis=1, keepdims=True)
            rowsum[rowsum == 0] = 1
            letter_models[letter].transitions = trans / rowsum

            # Update emissions
            emiss = accum[letter]['emiss']
            rowsum = emiss.sum(axis=1, keepdims=True)
            rowsum[rowsum == 0] = 1
            letter_models[letter].emissions = emiss / rowsum

            # Update null transitions
            total_null = sum(accum[letter]['null'].values())
            if total_null > 0:
                for (st1,st2), count in accum[letter]['null'].items():
                    if st2 == 'exit':
                        letter_models[letter].null_transitions[(st1,st2)] = count / total_null

        # Update SIL model
        sil_trans = accum['SIL']['trans']
        rowsum = sil_trans.sum(axis=1, keepdims=True)
        rowsum[rowsum == 0] = 1
        silence_model.transitions = sil_trans / rowsum

        sil_emiss = accum['SIL']['emiss']
        rowsum = sil_emiss.sum(axis=1, keepdims=True)
        rowsum[rowsum == 0] = 1
        silence_model.emissions = sil_emiss / rowsum

        # Update SIL null transitions
        total_null = sum(accum['SIL']['null'].values())
        if total_null > 0:
            for (st1,st2), count in accum['SIL']['null'].items():
                if st2 == 'exit':
                    silence_model.null_transitions[(st1,st2)] = count / total_null

        avg_log_prob = total_log_prob / total_frames if total_frames > 0 else 0
        log_likelihoods.append(total_log_prob)
        print(f"Iteration {iteration+1}:")
        print(f"  Total Log Likelihood: {total_log_prob:.2f}")
        print(f"  Avg Per-Frame Log Likelihood: {avg_log_prob:.4f}")

    return log_likelihoods

# Evaluation of models on test data
def test(test_labels, letter_models, silence_model, vocabulary):
    results = []
    word_models = {word: WordModel(word, letter_models, silence_model) for word in vocabulary}
    max_samples = len(test_labels)


    for i, lbls in enumerate(test_labels):
        print(f"\rProcessing sample {i+1}/{max_samples}", end="", flush=True)

        scores = []
        valid = True

        for word in vocabulary:
            model = word_models[word]
            model.build()
            try:
                _, scale = forward(lbls, model)
                # Calculating log likelihood, NOTE: i mistakenly calculated per_frame_score earlier. per_frame_score here corresponds to total score.
                total_score = np.sum(scale)
                #per_frame_score = total_score / len(lbls) if len(lbls) > 0 else 0
                per_frame_score = total_score
                scores.append((word, per_frame_score))
            except:
                valid = False
                break

        if not valid or not scores:
            continue

        # Normalizing scores
        log_scores = np.array([s for _, s in scores])
        log_sum = logsumexp(log_scores)
        confidences = np.exp(log_scores - log_sum)

        best_idx = np.argmax(log_scores)
        results.append({
            'word': scores[best_idx][0],
            'confidence': confidences[best_idx],
            'per_frame_ll': scores[best_idx][1],
            'top1': sorted(scores, key=lambda x: x[1], reverse=True)[:1]
            #'top3': sorted(scores, key=lambda x: x[1], reverse=True)[:3]
        })

    print()
    return results

# Main
def main():
    # Load data
    lblnames, train_words, train_labels, endpoints, test_labels, label_map = load_dataset()

    # Computing unigram counts for letters
    all_labels = [label for seq in train_labels for label in seq]
    unigram_counts = defaultdict(int)
    for label in all_labels:
        unigram_counts[label] += 1

    # Compute silence counts only from leading or trailing silences
    silence_counts = defaultdict(int)
    for seq, (i,j) in zip(train_labels, endpoints):
        for label in seq[:i+1] + seq[j:]:
            silence_counts[label] += 1

    # Initialize models
    letters = list("abcdefghijlmnoprstuvwxy")  # without k, q, z

    # Letter models all with exact same topology
    letter_models = {letter: HMM(3) for letter in letters}
    for letter in letters:
        letter_models[letter].initialize(unigram_counts)

    # SIL model with special topology - initialized with silence counts only
    silence_model = HMM(5, is_silence=True)
    silence_model.initialize(silence_counts)

    # Training models
    vocabulary = list(set(train_words))
    log_likelihoods = train(train_words, train_labels, letter_models, silence_model, iterations=25)

    # Plot the log-likelihood curve
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(log_likelihoods)+1), log_likelihoods, marker='o')
    plt.title("Training Log-Likelihood over Iterations")
    plt.xlabel("Iteration")
    plt.ylabel("Total Log-Likelihood (798 utterances)")
    plt.grid(True)
    plt.show()

    # Testing models
    test_results = test(test_labels, letter_models, silence_model, vocabulary)

    # Results
    print("\nTest Results:")
    for i, res in enumerate(test_results):
        print(f"\nTest {i+1}:")
        print(f"  Predicted: {res['word']}")
        print(f"  Confidence: {res['confidence']:.3f}")
        print(f"  Per-Frame Log Likelihood: {res['per_frame_ll']:.4f}")
        print("  Top candidate:")
        for word, score in res['top1']:
            print(f"    {word}: {score:.4f}")

if __name__ == "__main__":
    main()